\documentclass{letter}
\usepackage{graphicx}
\usepackage{color}
\date{Sep 11, 2015}

\newenvironment{review}%          environment name
{\textbf{Reviewer comment:}\begin{quote}}% begin code
{\end{quote}}%  

\newcommand{\todo}[1]{%                                                                                                                            
      \color{red}\textbf{[TODO]} #1\color{black}}

\newcommand{\answer}[1]{%                                                                                                                            
      \textbf{Answer:} #1}

\usepackage{hyperref}

\newcommand{\revised}[1]{\emph{#1}\color{black}}
\newcommand{\rev}[1]{\color{blue} #1\color{black}}

\begin{document}

\begin{letter}{}

\opening{Dear Editor and Reviewers,}

We would like to thank you for your thorough review and the useful
comments and suggestions formulated about our paper 
``Reproducibility of Execution Environments in Computational Science Using Semantics and Clouds'' 
submitted to Future Generation Computer Systems.

The remainder of this letter contains our answers to your reviews. 

We thank you again for your valuable review, and we remain available for any further information you may need.

\vspace{0.5cm}

Sincerely yours,

\vspace{1cm}

Idafen Santana-Perez, Rafael Ferreira da Silva, Mats Rynge, Ewa Deelman, Mar\'ia~S.~P\'erez-Hern\'andez, Oscar Corcho

\newpage


%
% Reviewer 1
%
\textbf{Reviewer 1}

% -----------------------------------------------
\begin{review}
What is reused from the previous works [21, 22] should be clarified, and the focus should be given on the new features (semantic modeling or algorithms).
\end{review}

\answer{We have reinforced the main contributions of this work with respect to our previous work, stating in the introduction new additions and modifications. We also have explained those contributions in their corresponding sections, including examples.}

%\revised{}


% -----------------------------------------------
\begin{review}
The structure of the paper could be clarified by, whenever possible, not mixing modeling and tooling aspects (beginning of 3.2.1, fig. 9 in 4.3). These modeling aspects should reinforce section 2.
\end{review}

\answer{In order to improve the readability and flow of this work, we have rearranged the disposition of sections and figures, based on the reviewer's suggestion. We also have suppressed some figures and listing that did not append meaningful insights (e.g., Fig.8 in the former version of the manuscript).}


% -----------------------------------------------
\begin{review}
The related works section does not provide a positioning of the approach, whereas it is clearly described in [22].
\end{review}

\answer{We have reviewed the  manuscript, in particular Section 5, highlighting the added value of our approach with respect to previous works on this research area.}


% -----------------------------------------------
\begin{review}
The paper does not provide a discussion of the approach and results. The discussion could describe and analyze what is automated and what is manual in the overall process. It would be very helpful to evaluate the cost of preserving a computing infrastructure associated to an in-silico experiment, in terms of semantic annotation of the workflow, the workflow management systems, the infrastructure, in terms also of software packaging in virtual machines.
\end{review}

\answer{We have add a \emph{Results and Discussion} section (Section 4.4), which highlights the costs and benefits of annotating a workflow execution environment. In addition, we discuss the different types of software components we faced in our experimentation process and how we deal with them.}


% -----------------------------------------------
\begin{review}
In addition, it would be interesting to have more information on the extensibility of this approach to other cloud middleware or workflow management systems.
\end{review}

\answer{We have stated in Section 6 that we are currently extending our approach to different workflow applications and systems. We have addressed workflow applications from other scientific domains (e.g., seismology), and analyzed two additional WMS. We have also investigated novel cloud scenarios and providers (e.g., containerization via Docker) in addition to the ones considered in this work.}


% -----------------------------------------------
\begin{review}
The perspectives could be reinforced with respect to the Linked Data movement, the needs to provide data management plans in large scale research projects, etc.
\end{review}

\answer{We have extended Section 4.2 to explain how our annotations conform to the Linked Data principles, and motivated how publishing and integrating these annotations could improve their reusability an understandability.}


% -----------------------------------------------
\begin{review}
Finally, I really appreciated the availability of the supplementary materials. Even if I could not complete the execution of the "epigenomics" workflow (timeout with 2 success jobs, and 67 unsubmitted jobs), I could easily deploy a local computing environment with Vagrant, and run one of the paper experiments.
\end{review}

\answer{We highly value that the experiment were able to be reproduced based on the experimental material provided. We have checked that they can be run in public providers, such as AWS and OpenStack-based platforms, whereas local reproduction using Vagrant may vary depending on the target computer. Several issues with RAM memory virtualization have been identified when working with Vagrant/VirtualBox solutions, which may cause timeouts in high-memory consuming workflows such as Epigenomics. We would be glad to investigate and address this specific case, as it is a valuable feedback to our work.}


% -----------------------------------------------
\begin{review}
Section 2: The content of this section does not clearly state if it is a contribution of this paper or not with respect to [21,22]. The new features should be clearly described. To complement what has been introduced in [21,22] a simple ``running'' example in the form of RDF triples (turtle syntax) could illustrate the expressivity of the WICUS vocabularies in terms of workflow annotations, software \& hardware constraints as well as available virtual appliances. It would also illustrate how these informations could be queried to automate the process of in silico experiment deployment.
\end{review}

\answer{We have clarified the new concepts of our models, included on the latest version of the WICUS ontology network. We have also included a running example summarizing the main RDF annotations of the SoyKB workflow (Listing 1), as well as an SPARQL query example for retrieving its software dependencies (Listing 2).}


% -----------------------------------------------
\begin{review}
Section 2: This semantic modeling section should also clearly show how the proposed vocabularies are related to other ones such as, for instance, P-PLAN or PROV for the workflow descriptions.
\end{review}

\answer{At the end of Section 2, we have added an explanation detailing which vocabularies have been aligned to the WICUS ontology network, introducing a simple example (\todo{add reference to example}).}


% -----------------------------------------------
\begin{review}
Section 3: Since the transformation catalog of Pegasus is part of the WICUS process fig 2, it should be better introduced in 3.1.
\end{review}

\answer{We have extended the description of the Pegasus' Transformation Catalog in Section 3.1.}


% -----------------------------------------------
\begin{review}
Section 3: The first part of 3.2.1 concerns the semantic modeling and not the tooling, it should be moved to section 2.
\end{review}

\answer{We have moved the beginning of Section 3.2.1 to Section 2, as we acknowledge that it improves the understandability of the semantics models description. We have also modified Section 3.2.1 to be consistent with these changes.}


% -----------------------------------------------
\begin{review}
Section 3: The 11 steps of 3.2.1 are not easy to follow. Maybe reducing the number of steps and focusing on the processing steps (DAX and TC annotator, Inf. Spec. Algorithm, Script generator), would ease the understanding. Also, it would be interesting to see what are the overall inputs of this workflow.
\end{review}

\answer{We have revised Figure 3 to reduce the number of steps and remove the intermediate files, so it is more concise and easy to follow. We have grouped the main inputs and outputs of the WICUS process flow. We have also rewritten the description of the process in order to ease its understanding.}


% -----------------------------------------------
\begin{review}
Section 3: In 3.2.2 ``interactions with the provisioned instances are done by tagging'', is not clear.
\end{review}

\answer{We have revised the sentence and clarified it:}

\revised{In PRECIP, when an instance is provisioned, the scientist can add arbitrary tags to that instance in order to identify and group the instances in the experiment. Then, future interactions can be performed by using the given tags.}


% -----------------------------------------------
\begin{review}
Section 4: This section instantiate the semantic models and run the WICUS process to reproduce 3 workflows. Since the ISA algorithm is part of the core of the approach, it should be introduced and described earlier.
\end{review}

\answer{We acknowledge that the description of the ISA should not be part of the experimentation section. Therefore, we have moved the description of the algorithm to Section 3, where we introduce the description of the WICUS modules and flow.}


% -----------------------------------------------
\begin{review}
Section 4: Figures 7 and 8 are not really informatives.
\end{review}

\answer{We have removed Figure 8 (description of the Epigenomics workflow annotations) since it is conceptually redundant to Figure 7. However, we kept Figure 7 (annotations about the Montage workflow) since we consider that it offers a more detailed explanation of how a concrete workflow is annotated, in contrast to Figure 6 where an overview is provided.}


% -----------------------------------------------
\begin{review}
Section 4: The use of terms "Image appliance", "Virtual appliance", "Scientific virtual appliance" are confusing. The sentence ``We define an Image Appliance that groups all VMs appliances into one single Scientific Virtual Appliance" is unclear.
\end{review}

\answer{In many cases, we used ``Virtual appliance'' and ``Scientific virtual appliance' as interchangeably, whereas ``Image Appliance'' is a different concept. We have revised the text to clarify the differences between these concepts when necessary. We have also added a brief explanation in Section 4 on how they relate to each other according to the WICUS-SVA domain ontology.}


% -----------------------------------------------
\begin{review}
Section 4: In 4.3 ``ISA selects the intersection where the value is maximized for each sub-workflow''. It is unclear, which value do you refer to? An example would be useful here.
\end{review}

\answer{We have added a more detailed explanation on how the SVAs are selected, specifying what we mean by the maximization of the intersection. In the selection process we retrieve the software components already available on each SVA as well as the dependency graph of each requirement and calculate their intersection. We select the SVAs that maximize this intersection.}


% -----------------------------------------------
\begin{review}
Section 4: Listing 1 : the text describing the pseudo code should refer to the lines of the listing. It is only true for lines 23 to 27.
\end{review}

\answer{We have included references to the lines on the pseudo code listing through the text description, which now belongs to Section 3.2.2.}


% -----------------------------------------------
\begin{review}
Section 4: The model of an Abstract Deployment Plan should be described in section 2.
\end{review}

\answer{We have moved the description of the deployment plan and its related ontology description to Section 2, as it is highly related to the semantic modeling.}


% -----------------------------------------------
\begin{review}
Section 4: The results of the experiment should constitute a single subsection 4.4.
\end{review}

\answer{We have created a new subsection (4.4) covering the results and discussion of our experimental process.}


% -----------------------------------------------
\begin{review}
Section 5: A positioning of the approach would reinforce the paper message, in particular with respect to the ReproZip approach which addresses the same objectives, and which also relies on Vagrant to automate the deployment of experiment.
\end{review}

\answer{We have revised and extended the related work section to include several positioning statements related to our approach and additional efforts. We have stated how our work differs from approaches such as RerpoZip, arguing that exposing the knowledge about the execution environment using semantics increases the reproducibility degree.}


% -----------------------------------------------
\begin{review}
Section 5: I don't see why Galaxy does not cover the local development of workflows. E-scientists can deploy a local galaxy server on their laptop and encapsulate or deploy binary softwares even outside the domain of ``omic'' data analysis.
\end{review}

\answer{Galaxy is mostly used for developing workflows involving external web services dependencies. This method does not fit into our approach, which describes local dependencies. We have revised the text to better reflect this argument.} \todo{Idafen, please check if my changes are ok for this answer.}


% -----------------------------------------------
\begin{review}
Section 5: Links with the generic parts of SWO and EDAM ontologies should be highlighted. Existing works on modeling Cloud based infrastructures could also be mentioned.
\end{review}

\answer{We have improved the comparison between our ontology network and the SWO and EDAM ontologies, as well as with some previous ontologies for Cloud Computing solutions.}


% -----------------------------------------------
\begin{review}
Section 6: From an e-science perspective, a summary of what is needed to implement a reproducible computing environment with WICUS would be helpful.
\end{review}

\answer{We have included several statements as part of the discussion and conclusions of this work, which describes the requirements that must be fulfilled in order to implement and share a reproducible environment. We also describe which information must be harvested and which skills are required to achieve the reproducibility of a workflow execution environment.}


% -----------------------------------------------
\begin{review}
Section 6: The results of the paper could also highlight what is stated in the introduction (``reproducibility implies that at least some changes have been introduced in the experiment''), by giving hints on how this approach ease e-scientists in changing some of the parameters of the experiments.
\end{review}

\answer{In Section 4.4 we have included a paragraph to explain what reproducibility means in the context of our approach and how it relates to the results of our experiments. Also, we state how it might be used by scientists to study the effects of modifying the execution environment.}


% -----------------------------------------------
\begin{review}
Section 6: Perspectives could be drawn on the benefits of publishing computing environment annotations as linked data.
\end{review}

\answer{We have added several statements introducing that our approach is generating annotations in conform to the Linked Data  principles (along with some examples), and discussing how its publication would improve its benefits.}


% -----------------------------------------------
\begin{review}
Section 6: ``...at the same time addresses the challenges of high storage demand of VM images.''; it should be clearly explained in the results section.
\end{review}

\answer{We acknowledge that this statement is bold. Therefore, we alleviated the boldness of the statement and explained how our approach tackles storage constraints by using public and generic images combined with the dynamic deployment approach, rather than generating a new VM image for each experiment.}


% -----------------------------------------------
\begin{review}
Section 6: "The drawback of our approach is that it assumes that the application ...  are publicly available.", it is rather a necessary condition to reproduce a software execution.
\end{review}

\answer{We acknowledge that this claim should be stated as a necessary condition rather than as a drawback. Thus, we have revised our claim and explained that the mentioned condition hold in most cases in the context of computational scientific experiments.}


% -----------------------------------------------
\begin{review}
A lot of references are URLs pointing to tools or projects. They should be added in the paper as footnotes.
\end{review}

\answer{We have changed the URL references to footnotes where necessary.}


% -----------------------------------------------
\begin{review}
the WMS requirements, and and one for each $\rightarrow$ the WMS requirements, and one for each \\
a well know workflow $\rightarrow$ a well known workflow \\
WICUS is a on-going effort $\rightarrow$ WICUS is aN on-going effort \\
for their valuable help $\rightarrow$ for their valuable help.
\end{review}

\answer{We have fixed these and other typos.}




\newpage

%
% Reviewer 2
%
\textbf{Reviewer 2}


\begin{review}
The description of the infrastructure, at least at the level of the experiment, is very basic (architecture, RAM, DISK). There are other important and complex features such as networks, additional disks, etc. that require both configuration and direct actions on the infrastructure provider. It is clear that the objective of the system is not to provide an environment for the deployment of customised virtual infrastructures, which could imply tens or hundreds of nodes with special configurations, but to instantiate a small-scale compatible infrastructure. The question is if this would suffice the execution of the experiments in regular scientific papers.
\end{review}

\answer{In this work, we cover single-node execution environments, arguing that they cover a wide range of computational environments. We have revised the manuscript in order to clarify and exemplify this consideration. We have then defined the scope of applicability in Section 1, and described how we plan to extend it to large and more complex infrastructures in Section 6. We have also discussed the hardware descriptions generated in this work within Section~2.}

\revised{}


% -----------------------------------------------
\begin{review}
Other important concern is the selection of the technologies. Currently, cloud infrastructures are becoming to adopt TOSCA specification and TOSCA templates to deploy and configure the environments. Comparison to WICUS should be made. Moreover, other DevOps tools such as cloudify (getcloudify.org), Infrastructure Manager (www.grycap.upv.es/im), cloudformation from AWS, to name a few, have moved from script-based configuration systems to declarative templates based on Ansible. These technological decisions should be analysed.
\end{review}

\answer{We have included several statements that discuss the technological decisions made for our system. We acknowledge the relevance of these solutions. Thus, we argue that most of them can be integrated into our system, as they provide an interesting and complementary approach to our work. In Section 3, we have introduced how the {\it Script Generator} component could be extended to support new cloud management systems. In Section 6, we have described several of them and stated the differences and commonalities with regard to our approach.}


% -----------------------------------------------
\begin{review}
The impact in disk of the VMIs is important, but it is not the bottleneck, but the data. Moreover, the use of containers will strongly mitigate this fact. A good management of base VM image and the use of containers for the different experiments may be efficient. Regarding the data, scientific experiments, and specially those dealing with genomics and astrophyiscs may require huge amounts of data, and transferring it back and forth may not be efficient. The integration of this solution with data preservation infrastructures can be studied.
\end{review}

\answer{\todo{RAFAEL: wf footprint is way heavier than the input data. We can also use containers, and still save space}}


\begin{review}
Finally, a major part of the article is dedicated to the description of the workflow use cases. Despite that the inclusion of three different use cases is interesting, effort should concentrate more on how they challenge the system. Additional information, such as specifications, and why not, actual reproducibility of the experiments in the article would be important.
\end{review}

\answer{\todo{why would be important to reproduce them: think and try to motivate...}}


\end{letter}
\end{document}
