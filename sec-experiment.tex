\section{Experimentation Process}
\label{sec:experiment}

In this section, we instantiate the semantic models introduced in 
Section~\ref{sec:reproducibility} (Figure~\ref{fig:wicusrels}) \rev{for real }
scientific workflow applications. This process is an extension of the one 
introduced in~\cite{SantanaPerez-REPPAR-2014}, in which we evaluate 
the improvements on our approach to the Montage~\cite{Montage} workflow.
\rev{In addition, we extend this evaluation to the Epigenomics\footnote{http://epigenome.usc.edu} 
and SoyKB~\cite{soybean, Joshi01012014} workflows}. We study and 
document these workflows and their execution environments, which include 
the application software components and the workflow management system.

The goal of this experiment is to reproduce original workflow executions 
on the three different Cloud scenarios: FutureGrid and Amazon EC2
using PRECIP, and a local execution environment by using Vagrant. 
FutureGrid is an academic Cloud testbed facility that includes a number 
of computational resources at distributed locations. Amazon Web Services 
EC2 is a public infrastructure provider, and the \emph{de facto} standard 
for IaaS Cloud platforms. \rev{Since most of Cloud platforms are based on
OpenStack\footnote{https://www.openstack.org}, our system can be used
by most of them}.


\subsection{Scientific Workflows}
\label{sec:workflows}

\paragraph{\textbf{Montage}}
The Montage workflow~\cite{Montage} was created by the NASA Infrared Processing 
and Analysis Center (IPAC) as an open source toolkit that can be used to generate 
custom mosaics of astronomical images in the Flexible Image Transport System (FITS) 
format. In a Montage workflow, the geometry of the output mosaic is calculated from the 
input images. The inputs are then re-projected to have the same spatial scale and rotation, 
the background emissions in the images are corrected to have a uniform level, and the 
re-projected, corrected images are co-added to form the output mosaic. 
Figure~\ref{fig:workflow-montage} illustrates a small (20 node) Montage workflow. The 
size of the workflow depends on the number of images required to construct the desired 
mosaic.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.85\linewidth]{figures/workflow-montage}
	\caption{A small (20 node) Montage workflow.}
	\label{fig:workflow-montage}
\end{figure}


\paragraph{\textbf{Epigenomics}}
The USC Epigenome Center is currently involved in mapping the epigenetic 
state of human cells on a genome-wide scale. The Epigenomics workflow 
(Figure~\ref{fig:workflow-genome}) processes multiple sets of genome sequences in
parallel. These sequences are split into subsets, the subsets are filtered to remove
contaminants, reformatted, and then mapped to a reference genome. The mapped sequences are
finally merged and indexed for later analysis. In this work, the Epigenomics workflow was 
used to align genome sequence reads to a reference genome for human chromosome 
21. The size of the workflow depends on the chunking factor used on the input data, 
which determines the number of sequence reads in each chunk.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.7\linewidth]{figures/workflow-genome}
	\caption{Epigenomics workflow.}
	\label{fig:workflow-genome}
\end{figure}

\paragraph{\textbf{SoyKB}}
The SoyKB workflow~\cite{soybean, Joshi01012014} is a genomics pipeline 
that re-sequences soybean germplasm lines selected for desirable traits such 
as oil, protein, soybean cyst nematode resistance, stress resistance, and root 
system architecture. The workflow (Figure~\ref{fig:workflow-soykb}) 
implements a Single Nucleotide Polymorphism (SNP)  and injection/deletion (indel) identification and analysis 
pipeline using the GATK haplotype caller\footnote{https://www.broadinstitute.org/gatk} and a soybean reference 
genome. The workflow analyzes samples in parallel to align them to the reference 
genome, to de-duplicate the data, to identify indels and SNPs, and to merge and 
filter the results. The results are then used for genome-wide association studies 
(GWAS) and genotype to phenotype analysis. The workflow instance used in this 
paper is based on a sample dataset that requires less memory than a full-scale 
production workflow, however it carries out the same process and 
	requires the same software components.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/workflow-soybean}
	\caption{SoyKB workflow.}
	\label{fig:workflow-soykb}
\end{figure}


\begin{figure*}[!htb]
	\centering
	\includegraphics[width=\linewidth]{figures/annotations}
	\vspace{-20pt}
	\caption{Overview of the generated annotations for the Montage, Epigenomics, and 
  	  SoyKB workflows using the WICUS ontology network (yellow rectangles represent the 
	  workflow component; blue squashed rectangles represent the Montage workflow; green 
	  bevelled rectangles represent the Epigenomics workflows; and red hexagons represent 
	  the SoyKB workflow).}
	\label{fig:annotations}
\end{figure*}


%%%
\subsection{Generating Semantic Annotations}

In this subsection, we present the annotations generated for each of the scientific 
workflows presented above and the Pegasus WMS using the WICUS ontology
network. All the annotations generated in this work are available through the 
Research Object (RO)~\cite{researchObjects} associated to this paper\footnote{http://purl.org/net/FGCS-Wicus-Pegasus-Repro}.

 As described in Figure~\ref{fig:wicusflow}, the first step in the process of 
documenting a workflow is the annotation of the workflow DAX file. We use the 
\texttt{Workflow} domain ontology to describe a workflow as 1) an individual that 
represents the top level workflow, and 2) a set of individuals representing its 
sub-workflows, one for each transformation. We then generate the necessary 
requirements, one for the top level workflow, which specifies the WMS requirements,
and one for each sub-workflow, which defines the software components required 
by each transformation. \rev{The data generated using this process follows the 
Linked Data~\cite{heath2011linked} principles. It uses HTTP URIs exposed 
in accordance with the RDF standards.}


Figure~\ref{fig:annotations} shows a simplified overview of the annotations generated 
using the WICUS ontology network for the Montage, Epigenomics, and SoyKB 
workflows as well as for the Pegasus WMS. Below, we describe each of these
semantic annotations in detail:


\paragraph{\textbf{Workflow Management System}}
We use the \texttt{Software} domain ontology to describe the components that
compose the workflow engine (in this case Pegasus) as individuals, and to 
represent its dependencies. Pegasus relies on \texttt{HTCondor} as the task manager, and 
both depend on \texttt{Java} and \texttt{wget}. In addition, all components also
depend on the operating system, which in our case is \texttt{CentOS}. The process
to describe the deployment of the WMS components is based on the installation of
and configuration processes as specified in their documentation. As a result, we will
define a set of installation scripts for each of the components. These scripts are 
included as part of the deployment plan along with their configuration information.
The WMS components are defined as a requirement (WMS Requirements) using
the \texttt{Workflow} domain ontology. This requirement is then linked to each
of the workflows included in this work. As a result, \texttt{Java}, \texttt{wget}, 
\texttt{HTCondor}, and \texttt{Pegasus WMS} should be installed on the target
computational resource.


\paragraph{\textbf{Montage Workflow}}
We use the \texttt{Workflow} domain ontology to describe the Montage workflow 
as an individual that represents the top level workflow, and another 9 individuals 
representing its sub-workflows, one for each transformation. We also generate 9 
requirements, which define the software components required by each transformation. 
At this point, these requirements are empty, as they are not yet related to their 
software components. Figure~\ref{fig:annotations-montage} shows the set of 
generated individuals for the Montage workflow.

Application components are described in the Montage workflow's Transformation 
Catalog, where the binary file, version, and destination path are defined. These 
components are also described as individuals using the \texttt{Software} domain 
ontology. We use this information to generate the configuration parameters of the 
deployment script, which in this case is the same for all components. The script 
downloads the binary files from an online repository and copies them to the specified 
destination path. This process identified 59 software components for the Montage 
workflow that are annotated and included in the Software Components Catalog.
Then, the Transformation Catalog Annotator module relates each transformation 
requirement, defined using the \texttt{Workflow} domain ontology, to the application 
component, and therefore to the deployment information. In this experiment, we 
define 9 Montage components that are linked to the requirements, and another two 
sub-components that are defined as dependencies in the software catalog 
(\emph{mDiffFit} depends on the \emph{mDiff} and \emph{mFitPlane} components).


\paragraph{\textbf{Epigenomics Workflow}}
Following the same approach as in the previous case, we use the \texttt{Workflow} 
domain ontology to describe the Epigenomics workflow as an individual that represents 
the top level workflow, and another 8 individuals representing its sub-workflows, one 
for each transformation. We then annotate 
the components described in the Epigenomics' Transformation Catalog as individuals 
using the \texttt{Software} domain ontology. We have also identified and annotated 6 
software dependencies related to the workflow, which include the Perl\footnote{https://www.perl.org} interpreter, 
the GNU libc\footnote{http://www.gnu.org/software/libc} and 
Libstdc++\footnote{http://gcc.gnu.org/libstdc++} libraries, and two other binaries from 
the Epigenomics  distribution, \texttt{maq} and \texttt{maqindex}.

\begin{figure}[!t]
	\centering
	\includegraphics[width=.85\linewidth]{figures/annotations-montage}
	\caption{Annotations for the Montage workflows using the \texttt{Workflow} domain ontology.}
	\label{fig:annotations-montage}
\end{figure}


%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=.85\linewidth]{figures/annotations-genome}
%	\caption{Annotations for the Epigenomics workflows using the \texttt{Workflow} domain ontology.}
%	\label{fig:annotations-genome}
%\end{figure}


\paragraph{\textbf{SoyKB Workflow}}
We describe the SoyKB workflow as an individual that represents the top level workflow,
and another 14 individuals representing each transformations. For the sake of simplicity,
we do not show the annotations for this workflow. Although SoyKB is the largest workflow 
in terms of its number of steps (around 670) among the other workflows included in this 
work, it defines only four software components as dependencies (\texttt{bwa-wrapper}, 
\texttt{gatk-wrapper}, \texttt{picard-wrapper}, and \texttt{software-wrapper}). These 
components  are software wrappers that invoke different libraries and binaries depending 
on the parameters used for the execution of the workflow. The components are included 
in a software bundle that is deployed on the computational nodes. Hence, a 
dependency for this bundle has been included in the Software 
Components Catalog.

\paragraph{\textbf{Computational Resources}}
We use the \texttt{Computing Resources} and \texttt{Hardware} domain ontologies 
to describe computational resources. For each Cloud resource (Amazon EC2 and 
FutureGrid), we defined two virtual machines: one that meets the requirements for the 
Montage and Epigenomics workflows (requires smaller disk space); and another for the 
SoyKB workflow (requires larger disk space). In both cases, we generated conceptually 
equivalent  appliances, as they both provide the CentOS 6 operating system, but differ
in the hardware configuration. We attempt to reduce the resource consumption of
Cloud resources due to the cost of storing/transferring VM images. Since Vagrant
execution is performed locally, we generated a single VM appliance that meets the 
requirements of all workflows. 

The description of the appliances is then included in the Scientific Virtual Appliances 
catalog (SVA, Figure~\ref{fig:wicusflow}). \rev{According to the SVA model, an 
SVA is supported by one or more Image Appliances, which represent the set of VMs
belonging to a single infrastructure provider that are able to enact the characteristics
(software and hardware) specified for the SVA. Thus, we created five Image Appliances 
that are grouped into a single Scientific Virtual Appliance (\texttt{CentOS 6 SVA}), 
which is defined by the CentOS 6 software stack}.
%Thus, we defined  an Image Appliance that groups 
%all image appliances into the \texttt{CentOS 6 SVA}.  will have five different Image Appliances 
%grouped into one single Scientific Virtual Appliance, defined by the CentOS 6 software stack}.
Table~\ref{tab:imgapps} summarizes the main characteristics of the five appliances 
we have annotated.

\begin{table}[!htb]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{8pt}
	\begin{tabular}{l | r r | r r | r}
		& \multicolumn{2}{c |}{Amazon EC2} & \multicolumn{2}{c |}{FutureGrid} & \multirow{2}{*}{Vagrant} \\
					& Small & Large & Small & Large \\ \hline
		RAM (GB) & 7 &  7 & 8 & 8 &  4 \\
		Disk (GB) 	&  40 &  5 &  40 & 5 & 50 \\
		CPU (GHz) & 2.4  & 2.4 & 2.9 & 2.9  &  2.83 \\
		CPU Arch. & \multicolumn{2}{c|}{64 bits} & \multicolumn{2}{c|}{64 bits} & \multicolumn{1}{c}{64 bits} \\
		OS & \multicolumn{2}{c|}{CentOS 6} & \multicolumn{2}{c|}{CentoOS 6} & \multicolumn{1}{c}{CentOS 6} \\
	\end{tabular}
	\caption{CentOS 6 Image Appliances.}
	\label{tab:imgapps}
\end{table}


\paragraph{\textbf{Hardware Requirements}}
For each scientific workflow, we have also analyzed the hardware specifications
required for their execution. Table~\ref{tab:hwreqs} shows the minimum threshold 
per workflow for each requirement. During the computational resource selection 
process (described in the following section), we will consider that any resource that
meets the requirements specified by a workflow will be a valid candidate for running 
the workflow. Since we do not target workflow execution performance, but a correct 
execution of the workflow, we have not identified any specific capacity regarding CPU
clock speed.

\begin{table}[!htb]
	\centering
	\footnotesize
	\setlength{\tabcolsep}{6pt}
	\begin{tabular}{l | c c r r}
 					& CPU (GHz) 	& CPU Arch. 	& RAM (GB)	& Disk (GB) \\ \hline
		Montage 		& -		 	& 64 bits 		& 4 			& 4 \\
		Epigenomics 	& - 			& 64 bits 		& 4 			& 4  \\
		SoyKB 		& -  			& 64 bits 		& 4 			& 10  \\
	\end{tabular}
	\caption{Workflow hardware requirements.}
	\label{tab:hwreqs}
\end{table}


%%%
\subsection{Reproducing Workflow Executions}

%\paragraph{\textbf{Execution Script}}

In all cases, \rev{after executing the ISA over the described annotations for the three workflows},
we were able to map the abstract plan either into a PRECIP or 
a Vagrant script (depending on the specified provider). Each generated script 
is composed of the following main sections:

\begin{itemize}
	\item \emph{Experiment Creation}: generates a new experiment using the 
		given VM image ID and the user credentials for the selected infrastructure 
		provider;
    	    
	\item \emph{Software Deployment}: executes the set of instructions defined on 
		the deployment plan of each software component to install and configure 
		the required software to execute the workflow. In this section, both the 
		workflow management system and the application are deployed with their 
		dependencies;

	\item \emph{User Setup}: creates a user account on the VM (if it does not exist) 
		and configures the necessary pair of SSH keys to enable file transfer and 
		execution. This account will be used to run the workflow;
	   
	\item \emph{Data Stage and Workflow Execution}: stages all the input data required 
		for the workflow execution on the VM, and launches the workflow execution. 
		Since our work is focused on infrastructure reproducibility, data and workflow 
		management are not covered in our approach. This part of the script is 
		generated ad-hoc for each workflow.
\end{itemize}

\noindent Note that all the configuration and deployment commands (first 3 sections) 
require superuser privileges on the VM. The workflow execution, however, is performed 
under the user account created in the third section.


% subsection
\rev{\subsection{Results and Discussion}}

We executed the resultant scripts for the three workflows over their corresponding 
platforms. That is, a total of 9 different executions, as each workflow is executed on  
Futuregrid and Amazon EC2 using PRECIP, and in a local Vagrant execution 
environment. All the executions were compared to their original one in a predefined 
VM image, where the execution environment was already in place.

Results show that the VM execution environments deployed by all scripts are able to
fully execute their related workflows. To check that not only the workflows are successfully executed but also 
that the results are correct and equivalent, we checked their produced output data. In the 
case of Montage, which produces an image as output, we used a perceptual hash 
tool\footnote{http://www.phash.org}
to compare the resulting image (0.1 degree image of the sky) against the one generated 
by the baseline execution, obtaining a similarity factor of 1.0 (over 1.0) with a threshold of 
0.85. In the Epigenomics and SoyKB workflows, the output data is non-deterministic due to
the existence of probabilistic steps. In this case, the use of a hash method is unfeasible. 
Hence, we validated the correct execution of the workflow by checking that correct output 
files were actually produced, and that the standard errors produced by the applications 
did not contain any error message. In both cases the results obtained in each infrastructure 
were equivalent in terms of their size (e.g., number of lines) and content.

%reviewer
%The paper does not provide a discussion of the approach and results. The discussion could describe and analyze what is automated and what is manual in the overall process. It would be very helpful to evaluate the cost of preserving a computing infrastructure associated to an in-silico experiment, in terms of semantic annotation of the workflow, the workflow management systems, the infrastructure, in terms also of software packaging in virtual machines.
%\note{not sure whether this should be here or in section 6, as it is part of the discussion}


\rev{
Experimental results show that scientific experiments in form of computational 
workflows can be reproduced based on semantic annotations using a proper 
vocabulary. The set of tools introduced in this work enables the generation 
of semantic annotations, which ease the effort of documenting the execution
environment. However, those tools do not provide a fully automated process,
thus some manual interactions are still required. Two of the entry files of the 
reproducibility process (\emph{WMS Annotations} and \emph{SVA Catalog}, 
Figure \ref{fig:wicusflow}) are manually generated. Although these files require
some effort to elaborate, they are created once and reused for all workflows
of a workflow management system}.
\note{Explain why would be challenging to automate the creation of these files. Maybe due to the need of specialized knowledge of the WMS. }
%These files are reused for the three experiments exposed in this section, and would be also used for new Pegasus workflows. 
\rev{
When reproducing workflows from different WMS, a new set of annotations 
would be required, whereas the \emph{SVA Catalog} could be reused. This 
catalog is extended as new computational resources are added to the system.
Most of the software involved on the execution of the three workflows studied 
in this work was, and still is, available online either through distribution packages 
repositories or hosted online by different institutions. Other software applications, 
such as the ones designed for the workflow, have been hosted in a public 
repository for this work. We decided to upload them to GItHub\footnote{https://github.com} 
to ensure that they could be retrieved remotely.
}

All the original and generated scripts are available as part of the experimental material 
included in the RO associated with this work. This RO also contains pointers to the software and resources 
used in the experimental evaluation.

\rev{The reproducibility of a computational environment presupposes that at 
least some differences may appear when compared to the former counterpart. 
In this work, we assume that these differences occur when using virtualization 
technologies and dynamic deployment of software components. Therefore, we 
obtain an equivalent infrastructure, which might not be an exact copy of the 
original one, but exposes the required characteristics for executing the workflow. 
This approach allows scientists to understand how the underlying infrastructure 
is defined, and to evaluate the impact of different configurations on the 
experiment, as the configuration parameters of the environment are explicitly 
defined in the semantic annotations.}


%\begin{listing}[!htb]
%\begin{minted}[linenos=true,fontsize=\scriptsize]{cpp}
%
%OPEN_SSH_CLIENTS_SOFT_STACK stack
% OPEN_SSH_CLIENTS_SOFT_COMP component
%  OPEN_SSH_CLIENTS_DEP_STEP step
%OPEN_SSH_SERVER_SOFT_STACK stack
% OPEN_SSH_SERVER_SOFT_COMP component
%  OPEN_SSH_SERVER_DEP_STEP step
%WGET_SOFT_STACK stack
% WGET_SOFT_COMP component
%  WGET_DEP_STEP step
%CONDOR_CENTOS_6_5_SOFT_STACK stack
% CONDOR_CENTOS_6_5_SOFT_COMP component
%  STOP_CONDOR_DEP_STEP step
%  ADD_CONDOR_REPO_DEP_STEP step
%  CONDOR_YUM_INSTALL_DEP_STEP step
%  CLEAN_AND_SET_CONDOR_DEP_STEP step
%  RESTART_DAEMONS_DEP_STEP step
%JAVA-1.7.0-OPENJDK.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK.X86_64_DEP_STEP step
%JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK-DEVEL.X86_64_DEP_STEP step
%PEGASUS_WMS_CENTOS_6_5_SOFT_STACK stack
% PEGASUS_WMS_CENTOS_6_5_SOFT_COMP component
%  ADD_PEGASUS_REPO_DEP_STEP step
%  PEGASUS_YUM_INSTALL_DEP_STEP step
%MFITPLANE_SOFT_STACK stack
% MFITPLANE_SOFT_COMP component
%  MFITPLANE_DEP_STEP step
%MDIFF_SOFT_STACK stack
% MDIFF_SOFT_COMP component
%  MDIFF_DEP_STEP step
%MDIFFFIT_SOFT_STACK stack
% MDIFFFIT_SOFT_COMP component
%  MDIFFFIT_DEP_STEP step
%MCONCATFIT_SOFT_STACK stack
% MCONCATFIT_SOFT_COMP component
%  MCONCATFIT_DEP_STEP step
%MIMGTBL_SOFT_STACK stack
% MIMGTBL_SOFT_COMP component
%  MIMGTBL_DEP_STEP step
%MSHRINK_SOFT_STACK stack
% MSHRINK_SOFT_COMP component
%  MSHRINK_DEP_STEP step
%MPROJECTPP_SOFT_STACK stack
% MPROJECTPP_SOFT_COMP component
%  MPROJECTPP_DEP_STEP step
%MADD_SOFT_STACK stack
% MADD_SOFT_COMP component
%  MADD_DEP_STEP step
%MBACKGROUND_SOFT_STACK stack
% MBACKGROUND_SOFT_COMP component
%  MBACKGROUND_DEP_STEP step
%MBGMODEL_SOFT_STACK stack
% MBGMODEL_SOFT_COMP component
%  MBGMODEL_DEP_STEP step
%MJPEG_SOFT_STACK stack
% MJPEG_SOFT_COMP component
%  MJPEG_DEP_STEP step
%
%\end{minted}
%\caption{Abstract deployment plan of the Montage WF}
%\label{lst:plan-montge}
%\end{listing}
%
%
%\begin{listing}[!htb]
%\begin{minted}[linenos=true,fontsize=\scriptsize]{cpp}
%
%OPEN_SSH_CLIENTS_SOFT_STACK stack
% OPEN_SSH_CLIENTS_SOFT_COMP component
%  OPEN_SSH_CLIENTS_DEP_STEP step
%OPEN_SSH_SERVER_SOFT_STACK stack
% OPEN_SSH_SERVER_SOFT_COMP component
%  OPEN_SSH_SERVER_DEP_STEP step
%WGET_SOFT_STACK stack
% WGET_SOFT_COMP component
%  WGET_DEP_STEP step
%CONDOR_CENTOS_6_5_SOFT_STACK stack
% CONDOR_CENTOS_6_5_SOFT_COMP component
%  STOP_CONDOR_DEP_STEP step
%  ADD_CONDOR_REPO_DEP_STEP step
%  CONDOR_YUM_INSTALL_DEP_STEP step
%  CLEAN_AND_SET_CONDOR_DEP_STEP step
%  RESTART_DAEMONS_DEP_STEP step
%JAVA-1.7.0-OPENJDK.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK.X86_64_DEP_STEP step
%JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK-DEVEL.X86_64_DEP_STEP step
%PEGASUS_WMS_CENTOS_6_5_SOFT_STACK stack
% PEGASUS_WMS_CENTOS_6_5_SOFT_COMP component
%  ADD_PEGASUS_REPO_DEP_STEP step
%  PEGASUS_YUM_INSTALL_DEP_STEP step
%MAQ_INDEX_X86_64_SOFT_STACK stack
% MAQ_INDEX_X86_64_SOFT_COMP component (version: 1.0)
%  MAQ_INDEX_X86_64_DEP_STEP step
%  MAQ_INDEX_X86_64_2_DEP_STEP step
%MAQ_X86_64_SOFT_STACK stack
% MAQ_X86_64_SOFT_COMP component (version: 1.0)
%  MAQ_X86_64_DEP_STEP step
%  MAQ_X86_64_2_DEP_STEP step
%RPM_GLIBCi686_SOFT_STACK stack
% RPM_GLIBCi686_SOFT_COMP component
%  RPM_GLIBCi686_DEP_STEP step
%RPM_GLIBC_X86_64_SOFT_STACK stack
% RPM_GLIBC_X86_64_SOFT_COMP component
%  RPM_GLIBC_X86_64_DEP_STEP step
%RPM_COMPAT_LIBSTDCPP_33_X86_64_SOFT_STACK stack
% RPM_COMPAT_LIBSTDCPP_33_X86_64_SOFT_COMP component
%  RPM_COMPAT_LIBSTDCPP_33_X86_64_DEP_STEP step
%RPM_PERL_V5_14_4_X86_64_SOFT_STACK stack
% RPM_PERL_V5_14_4_X86_64_SOFT_COMP component
%  RPM_PERL_V5_14_4_X86_64_DEP_STEP step
%FASTQSPLIT_SOFT_STACK stack
% FASTQSPLIT_SOFT_COMP component (version: 1.0)
%  FASTQSPLIT_DEP_STEP step
%  FASTQSPLIT_2_DEP_STEP step
%SOL2SANGER_SOFT_STACK stack
% SOL2SANGER_SOFT_COMP component (version: 1.0)
%  SOL2SANGER_DEP_STEP step
%  SOL2SANGER_2_DEP_STEP step
%MAP_SOFT_STACK stack
% MAP_SOFT_COMP component (version: 1.0)
%  MAP_DEP_STEP step
%  MAP_2_DEP_STEP step
%CHR21_SOFT_STACK stack
% CHR21_SOFT_COMP component (version: 1.0)
%  CHR21_DEP_STEP step
%  CHR21_2_DEP_STEP step
%PILEUP_SOFT_STACK stack
% PILEUP_SOFT_COMP component (version: 1.0)
%  PILEUP_DEP_STEP step
%  PILEUP_2_DEP_STEP step
%FAST2BFQ_SOFT_STACK stack
% FAST2BFQ_SOFT_COMP component (version: 1.0)
%  FAST2BFQ_DEP_STEP step
%  FAST2BFQ_2_DEP_STEP step
%MAPMERGE_SOFT_STACK stack
% MAPMERGE_SOFT_COMP component (version: 1.0)
%  MAPMERGE_DEP_STEP step
%  MAPMERGE_2_DEP_STEP step
%FILTERCONTAMS_SOFT_STACK stack
% FILTERCONTAMS_SOFT_COMP component (version: 1.0)
%  FILTERCONTAMS_DEP_STEP step
%  FILTERCONTAMS_2_DEP_STEP step
%
%\end{minted}
%\caption{Abstract deployment plan of the Epigenomics WF}
%\label{lst:plan-epigenomics}
%\end{listing}