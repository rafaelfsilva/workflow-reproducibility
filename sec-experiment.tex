\section{Experimental Evaluation}
\label{sec:experiment}

In this section, we conduct a practical evaluation through experimentation in 
which we instantiate the semantic models (Figure~\ref{fig:wicusrels}) for three real 
scientific workflow applications. This process is an extension of the one introduced
 in~\cite{SantanaPerez-REPPAR-2014}, in which we evaluate the the improvements on our
 approach to the Montage~\cite{Montage} workflow, and also evaluate the
 Epigenomics~\cite{genome}, and SoyKB~\cite{soybean, Joshi01012014} workflows.
We study and document these workflows and their execution environments, which include the application software components 
and the workflow management system.

The goal of this experiment is to reproduce original workflow executions into the three different 
Cloud scenarios introduces in this work: FutureGrid~\cite{futuregrid} and Amazon EC2~\cite{aws} 
using PRECIP, and a local execution environment by using Vagrant. 
FutureGrid is an academic Cloud test-bed facility that includes a number of computational 
resources at distributed locations. Amazon Web Services EC2 is a public infrastructure 
provider and the \emph{de facto} standard for IaaS Cloud platforms. 


\subsection{Scientific Workflows}

\paragraph{\textbf{Montage}}
The Montage workflow~\cite{Montage} was created by the NASA Infrared Processing 
and Analysis Center (IPAC) as an open source toolkit that can be used to generate 
custom mosaics of astronomical images in the Flexible Image Transport System (FITS) 
format. In a Montage workflow, the geometry of the output mosaic is calculated from the 
input images. The inputs are then re-projected to have the same spatial scale and rotation, 
the background emissions in the images are corrected to have a uniform level, and the 
re-projected, corrected images are co-added to form the output mosaic. 
Figure~\ref{fig:workflow-montage} illustrates a small (20 node) Montage workflow. The 
size of the workflow depends on the number of images required to construct the desired 
mosaic.

\begin{figure}[!htt]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/workflow-montage}
	\caption{A small (20 node) Montage workflow.}
	\label{fig:workflow-montage}
\end{figure}


\paragraph{\textbf{Epigenomics}}
The USC Epigenome Center~\cite{genome} is currently involved in mapping the epigenetic 
state of human cells on a genome-wide scale. The Epigenomics workflow 
(Figure~\ref{fig:workflow-genome}) processes multiple sets of genome sequences in
parallel. These sequences are split into subsets, the subsets are filtered to remove
contaminants, reformatted, then mapped to a reference genome. The mapped sequences are
finally merged and indexed for later analysis. In this work, the Epigenome workflow was 
used to align genome sequence reads to a reference genome for human chromosome 
21. The size of the workflow depends on the chunking factor used on the input data, 
which determines the number of sequence reads in each chunk.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\linewidth]{figures/workflow-genome}
	\caption{Epigenomics workflow.}
	\label{fig:workflow-genome}
\end{figure}

\paragraph{\textbf{SoyKB}}
The SoyKB workflow~\cite{soybean, Joshi01012014} is a genomics pipeline 
that re-sequences soybean germplasm lines selected for desirable traits such 
as oil, protein, soybean cyst nematode resistance, stress resistance, and root 
system architecture. The workflow (Figure~\ref{fig:workflow-soykb}) 
implements a SNP and injection/deletion (indel) identification and analysis 
pipeline using the GATK haplotype caller~\cite{gatk} and a soybean reference 
genome. The workflow analyzes samples in parallel to align them to the reference 
genome, to de-duplicate the data, to identify indels and SNPs, and to merge and 
filter the results. The results are then used for genome-wide association studies 
(GWAS) and genotype to phenotype analysis. The workflow instance used in this 
paper is based on a sample dataset that requires less memory than a full-scale 
production workflow, however it requires the same software components.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/workflow-soybean}
	\caption{SoyKB workflow.}
	\label{fig:workflow-soykb}
\end{figure}



%%%
\subsection{Generating Semantic Annotations}

In this subsection, we present the annotations generated for each of the scientific 
workflows presented above and the Pegasus WMS using the WICUS ontology
network. As described in Figure~\ref{fig:wicusflow}, the first step in the process of 
documenting a workflow is the annotation of the workflow DAX file. We use the 
\texttt{Workflow} domain ontology to describe a workflow as 1) an individual that 
represents the top level workflow, and 2) a set of individuals representing its 
sub-workflows, one for each transformation. We then generate the necessary 
requirements, one for the top level workflow, which specifies the WMS requirements, 
and one for each sub-workflow, which defines the software components required 
by each transformation. 
Figure~\ref{fig:annotations} shows a simplified overview of the annotations generated 
using the WICUS ontology network for the Montage, Epigenomics, and SoyKB 
workflows as well as for the Pegasus WMS. Below, we describe each of these
semantic annotations in details:

\begin{figure*}[!htb]
	\centering
	\includegraphics[width=\linewidth]{figures/annotations}
	\vspace{-20pt}
	\caption{Overview of the generated annotations for the Montage, Epigenomics, and 
  	  SoyKB workflows using the WICUS ontology network (yellow rectangles represent the 
	  workflow component; blue squashed rectangles represent the Montage workflow; green 
	  bevelled rectangles represent the Epigenomics workflows; and red hexagons represent 
	  the SoyKB workflow). \note{The figure does not take into account Vagrant.}}
	\label{fig:annotations}
\end{figure*}


\paragraph{\textbf{Workflow Management System}}
We use the \texttt{Software} domain ontology to describe the components that
compose the workflow engine (in this case Pegasus) as individuals, and to 
represent its dependencies. Pegasus relies on \texttt{HTCondor} as task manager, and 
both depend on \texttt{Java} and \texttt{wget}. In addition, all components also
depend on the operating system, which in our case is \texttt{CentOS}. The process
to describe the deployment of the WMS components is based on the installation 
and configuration processes as specified in their documentation. As a result, we 
defined a set of installation scripts for each of the components. These scripts are 
included as part of the deployment plan along with their configuration information.
The WMS components are defined as a requirement (WMS Requirements) using
the \texttt{Workflow} domain ontology. This requirement is then linked to each
of the workflows included in this work. As a result, \texttt{Java}, \texttt{wget}, 
\texttt{HTCondor}, and \texttt{Pegasus WMS} should be installed on the target
computational resource.


\paragraph{\textbf{Montage Workflow}}
We use the \texttt{Workflow} domain ontology to describe the Montage workflow 
as an individual that represents the top level workflow, and another 9 individuals 
representing its sub-workflows, one for each transformation. We also generate 9 
requirements, which define the software components required by each transformation. 
At this point, these requirements are empty, as they are not yet related to their 
software components. Figure~\ref{fig:annotations-montage} shows the set of 
generated individuals for the Montage workflow.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=.7\linewidth]{figures/annotations-montage}
	\vspace{-10pt}
	\caption{Annotations for the Montage workflows using the \texttt{Workflow} domain ontology.}
	\label{fig:annotations-montage}
	\note{Rafael, should we also include the WMS reqs related to the Montage WF individual?}
\end{figure}

Application components are described in the Montage workflow's Transformation 
Catalog, where the binary file, version, and destination path are defined. These 
components are also described as individuals using the \texttt{Software} domain 
ontology. We use this information to generate the configuration parameters of the 
deployment script, which in this case is the same for all components. The script 
downloads the binary files from an online repository and copies them to the specified 
destination path. This process identified 59 software components for the Montage 
workflow that are annotated and included in the Software Components Catalog.
Then, the Transformation Catalog Annotator module relates each transformation 
requirement, defined using the \texttt{Workflow} domain ontology, to the application 
component, and therefore to the deployment information. In this experiment, we 
define 9 Montage components that are linked to the requirements, and another two 
sub-components that are defined as dependencies in the software catalog 
(\emph{mDiffFit} depends on the \emph{mDiff} and \emph{mFitPlane} components).


\paragraph{\textbf{Epigenomics Workflow}}
Following the same approach as in the previous case, we use the \texttt{Workflow} 
domain ontology to describe the Epigenomics workflow as an individual that represents 
the top level workflow, and another 8 individuals representing its sub-workflows, one 
for each transformation (Figure~\ref{fig:annotations-genome}). We have then annotated 
the components described in the Epigenomics' Transformation Catalog as individuals 
using the \texttt{Software} domain ontology. We have also identified and annotated 6 
software dependencies related to the workflow, which include the Perl~\cite{perl} interpreter, 
the GNU libc~\cite{libc} and Libstdc++~\cite{libstdc} libraries, and two other binaries from 
the Epigenomics  distribution, \texttt{maq} and \texttt{maqindex}.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=.7\linewidth]{figures/annotations-genome}
	\vspace{-10pt}
	\caption{Annotations for the Epigenomics workflows using the \texttt{Workflow} domain ontology.}
	\label{fig:annotations-genome}
	\note{Rafael, should we also include the WMS reqs related to the Epigenomics WF individual?}
\end{figure}


\paragraph{\textbf{SoyKB Workflow}}
We describe the SoyKB workflow as an individual that represents the top level workflow,
and another 14 individuals representing each transformations. For the sake of simplicity,
we do not show the annotations for this workflow. Although SoyKB is the largest workflow 
in terms of its number of steps (around 670) among the other workflows included in this 
work, it defines only four software components as dependencies (\texttt{bwa-wrapper}, 
\texttt{gatk-wrapper}, \texttt{picard-wrapper}, and \texttt{software-wrapper}). These 
components  are software wrappers that invoke different libraries and binaries depending 
on the parameters used for the execution of the workflow. The components are included 
on a software bundle that is deployed on the computational nodes to be invoked by the 
wrappers. Hence, a dependency for this bundle has been included in the Software 
Components Catalog.


\paragraph{\textbf{Computational Resources}}
We use the \texttt{Computing Resources} and \texttt{Hardware} domain ontologies 
to describe computational resources. For each Cloud resource (Amazon EC2 and 
FutureGrid), we defined two virtual machines: one that meets the requirements for the 
Montage and Epigenomics workflows (requires smaller disk space); and one for the 
SoyKB workflow (requires larger disk space). In both cases, we generated conceptually 
equivalent VMs appliances, as they both provide CentOS 6 operating system, but differed 
in the hardware configuration. We attempt to reduce the resource consumption of
Cloud resources due to the cost for storing/transferring VM images. Since Vagrant
execution is performed locally, we generated a single VM appliance that meets the 
requirements of all workflows. 

The description of the appliances is then included in the Scientific Virtual Appliances 
Catalog (step 7 in Figure~\ref{fig:wicusflow}). We define  an Image Appliance that groups 
all VMs appliances into one single Scientific Virtual Appliance (\texttt{CentOS 6 SVA}). 
As described in the Software domain ontology~\cite{wicus} a Scientific Virtual Appliance
is defined by the set of software elements associated to it, while Image Appliances 
describe the combination of hardware characteristics and infrastructure provider. Thus, 
we will have five different Image Appliances grouped into one single Scientific Virtual Appliance,
defined by the Centos 6 software stack.
Table~\ref{tab:imgapps} summarizes the main characteristics of the five appliances 
we have annotated.

\begin{table}[!htb]
	\centering
	\scriptsize
	\setlength{\tabcolsep}{8pt}
	\begin{tabular}{l | r r | r r | r}
		& \multicolumn{2}{c |}{Amazon EC2} & \multicolumn{2}{c |}{FutureGrid} & \multirow{2}{*}{Vagrant} \\
					& Small & Large & Small & Large \\ \hline
		RAM (GB) & 7 &  7 & 8 & 8 &  4 \\
		Disk (GB) 	&  40 &  5 &  40 & 5 & 50 \\
		CPU (GHz) & 2.4  & 2.4 & 2.9 & 2.9  &  2.7 \\
		CPU Arch. & \multicolumn{2}{c|}{64 bits} & \multicolumn{2}{c|}{64 bits} & \multicolumn{1}{c}{64 bits} \\
		OS & \multicolumn{2}{c|}{CentOS 6} & \multicolumn{2}{c|}{CentoOS 6} & \multicolumn{1}{c}{CentOS 6} \\
	\end{tabular}
	\caption{CentOS 6 Virtual Image Appliances.}
	\label{tab:imgapps}
\end{table}


\paragraph{\textbf{Hardware Requirements}}
For each scientific workflow, we have also analyzed the hardware specifications
required for their execution. Table~\ref{tab:hwreqs} shows the minimum threshold 
per workflow for each requirement. During the computational resource selection 
process (described in the following section), we will consider that any resource that
meets the requirements specified by a workflow will be a valid candidate for running 
the workflow. Since we do not target workflow execution performance, but a correct 
execution of the workflow, we have not identified any specific capacity regarding CPU
frequency.

\begin{table}[!htb]
	\centering
	\scriptsize
	\setlength{\tabcolsep}{7pt}
	\begin{tabular}{l | c c r r}
 					& CPU (GHz) 	& CPU Arch. 	& RAM (GB)	& Disk (GB) \\ \hline
		Montage 		& -		 	& 64 bits 		& 4 			& 4 \\
		Epigenomics 	& - 			& 64 bits 		& 4 			& 4  \\
		SoyKB 		& -  			& 64 bits 		& 4 			& 10  \\
	\end{tabular}
	\caption{Workflow hardware requirements.}
	\label{tab:hwreqs}
\end{table}


%%%
\subsection{Reproducing Workflow Executions.}

The last step on the process for achieving reproducibility in scientific workflows 
(Figure~\ref{fig:wicusflow}) is to execute the Infrastructure Specification Algorithm 
(ISA)~\cite{wicus}. The algorithm retrieves the corresponding information for the 
workflow and its dependencies from the annotations datasets, and calculates the 
dependencies and compatibility between requirements and the available 
computational resources. It also considers the software already installed on the
resources to avoid unnecessary installation steps.

\paragraph{\textbf{Infrastructure Specification Algorithm}}
ISA combines the annotated data based on the 4 
domain ontologies in order to find a suitable infrastructure specification that meets 
the requirements of the workflow. The algorithm retrieves and propagates the WMS 
requirements of the top-level workflow (\texttt{Workflow} domain ontology) to its 
related sub-workflows (as defined in Figure~\ref{fig:annotations}). Requirements 
and software components are matched, and a dependency graph is built based 
on the relation between the requirements and the component dependencies. This 
graph is then used to compute the intersection between the set of software components 
from the SVA and the dependency graph of each sub-workflow. ISA selects the 
intersection where the value is maximized for each sub-workflow. Software components 
already available in the SVA are then removed from the chosen graph. To reduce 
the number of SVAs, the algorithm attempts to merge sub-workflows requirements 
into a single SVA. Requirements can be merged if all their software components are 
compatible. Finally, ISA generates a script (either using PRECIP or Vagrant) with 
the set of required instructions to instantiate, configure, and deploy the computational 
resources and software components on the corresponding provider. Listing~\ref{lst:pseudo}
shows a pseudo-code of the algorithm.
          
\begin{lstlisting}[caption={Pseudo-code overview of the Infrastructure Specification Algorithm (ISA).},label={lst:pseudo}]
WorkflowRequirementsDataset.load();

SVADataset.load();

SoftwareCatalogDataset.load();

Map<Workflow,List<Requirements>> wfSwReqs = retrieveSwRequirements( WorkflowRequirementsDataset, WorkflowID );

Map<Workflow,List<Requirements>> propagatedWfSwReqs = propagateSwReqs( wfSwReqs );

List<List<List<SWComponents>>> softwareComponents = getSoftwareComponents( propagatedWfSwReqs );

Map<Requirement,D-Graph<SWComponents>> softwareComponentsDependencyGraph =    getSoftwareDependencies(softwareComponents );

List<SVA> availableSvas = getAvailableSvas(providersList);

Map<Requirements,SVA> maxCompatibilities = getCompatibilityIntersection( softwareComponentsDependencyGraph, availableSvas );

Map<Requirement,D-Graph<SWComponents>> substractedSwComponentsDepGraph =  substractSoftwareComponents( softwareComponentsDependencyGraph, maxCompatibilities );

Map<SVA, List<Requirments>>mergedSvas= mergeSubworkflows( propagatedWfSwReqs, maxCompatibilities );

Map<Workflow,List<Requirements>> wfHwReqs =  retrieveHwRequirements( WorkflowRequirementsDataset, WorkflowID );

Map<SVA, List<Requirments>>filteredSvas= getCompatibleHwImageAppliances( mergedSvas, wfHwReqs );

generateScript( filteredSvas, substractedSwComponentsDepGraph );
\end{lstlisting}

In this work, we have extended ISA to support 1)~Image Appliance filtering, and 
2) generation of Vagrant scripts. The algorithm filters Image Appliances from the 
selected SVAs that do not meet the hardware requirements specified for the workflow
(lines 23--25). To enact support to different execution scripts, we created an intermediate 
phase (\emph{Abstract Deployment Plan}, step 9 in Figure~\ref{fig:wicusflow}), which 
defines the steps and scripts to be executed, along with their configuration parameters. 
ISA then concretizes this plan into a PRECIP or Vagrant script depending on the resultant 
target provider (line 27).


\paragraph{\textbf{Abstract Deployment Plan}}
This layer allows WICUS to generate abstract deployment plans regardless of the
underlying execution tool. The abstract plan is based on the WICUS Software~\cite{wicus} 
domain ontology, which defines the software stacks that should be deployed in the 
execution platform. Figure~\ref{fig:stack-rel} shows the relations between the different
elements that compose the \texttt{Stack} domain ontology. A \texttt{Software Stack} may
be composed by one or more \texttt{Software Components}. Each of them has an associated 
\texttt{Deployment Plan} according to the target execution platform, which is composed by 
one or more \texttt{Deployment Steps}.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/stack-rel}
	\caption{Overview of the WICUS software stack relation diagram.}
	\label{fig:stack-rel}
\end{figure}

Listing \ref{lst:plan-soykb} shows an example of the abstract plan for the SoyKB workflow. 
The first section of the plan (lines 1--26) describes the deployment of the \texttt{Pegasus 
WMS} and its related dependencies. Note that this section is common across all deployment
plans for the workflows covered in this work. The remaining lines describe how the SoyKB 
software is deployed. The \texttt{SOFTWARE.TAR.GZ} stack, which is a dependency for all 
SoyKB wrappers, is the first component to be deployed (lines 27--29). Finally, the last section 
of the plan (lines 30--45) describes how the four SoyKB wrappers are deployed. For each
wrapper, two deployment steps are required: 1)~copy of the program execution binary, and
2)~the granting of proper execution permissions.

\begin{lstlisting}[caption={Abstract deployment plan of the SoyKB WF.}, label={lst:plan-soykb}]
OPEN_SSH_CLIENTS_SOFT_STACK stack
  OPEN_SSH_CLIENTS_SOFT_COMP component
    OPEN_SSH_CLIENTS_DEP_STEP step
OPEN_SSH_SERVER_SOFT_STACK stack
  OPEN_SSH_SERVER_SOFT_COMP component
    OPEN_SSH_SERVER_DEP_STEP step
WGET_SOFT_STACK stack
  WGET_SOFT_COMP component
    WGET_DEP_STEP step
CONDOR_CENTOS_6_5_SOFT_STACK stack
  CONDOR_CENTOS_6_5_SOFT_COMP component
    STOP_CONDOR_DEP_STEP step
    ADD_CONDOR_REPO_DEP_STEP step
    CONDOR_YUM_INSTALL_DEP_STEP step
    CLEAN_AND_SET_CONDOR_DEP_STEP step
    RESTART_DAEMONS_DEP_STEP step
JAVA-1.7.0-OPENJDK.X86_64_SOFT_STACK stack
  JAVA-1.7.0-OPENJDK.X86_64_SOFT_COMP component
    JAVA-1.7.0-OPENJDK.X86_64_DEP_STEP step
JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_STACK stack
  JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_COMP component
    JAVA-1.7.0-OPENJDK-DEVEL.X86_64_DEP_STEP step
PEGASUS_WMS_CENTOS_6_5_SOFT_STACK stack
  PEGASUS_WMS_CENTOS_6_5_SOFT_COMP component
    ADD_PEGASUS_REPO_DEP_STEP step
    PEGASUS_YUM_INSTALL_DEP_STEP step
SOFTWARE_TAR_GZ_SOFT_STACK stack
  SOFTWARE_TAR_GZ_SOFT_COMP component
    SOFTWARE_TAR_GZ_DEP_STEP step
PICARD-WRAPPER_SOFT_STACK stack
  PICARD-WRAPPER_SOFT_COMP component
    PICARD-WRAPPER_DEP_STEP step
    PICARD-WRAPPER_2_DEP_STEP step
SOFTWARE-WRAPPER_SOFT_STACK stack
  SOFTWARE-WRAPPER_SOFT_COMP component
    SOFTWARE-WRAPPER_DEP_STEP step
    SOFTWARE-WRAPPER_2_DEP_STEP step
GATK-WRAPPER_SOFT_STACK stack
  GATK-WRAPPER_SOFT_COMP component
    GATK-WRAPPER_DEP_STEP step
    GATK-WRAPPER_2_DEP_STEP step
BWA-WRAPPER_SOFT_STACK stack
  BWA-WRAPPER_SOFT_COMP component
    BWA-WRAPPER_DEP_STEP step
    BWA-WRAPPER_2_DEP_STEP step
\end{lstlisting}


\paragraph{\textbf{Execution Script}}
In all cases, ISA is able to concretize the abstract plan either as a PRECIP or 
a Vagrant script (depending on the specified provider). Each generated script 
is composed by the following main sections:

\begin{itemize}
	\item \emph{Experiment Creation}: generates a new experiment using the 
		given VM image ID and the user credentials for the selected infrastructure 
		provider;
    	    
	\item \emph{Software Deployment}: executes the set of instructions defined on 
		the deployment plan of each software component to install and configure 
		the required software to execute the workflow. In this section, both the 
		workflow management system and the application are deployed with their 
		dependencies;

	\item \emph{User Setup}: creates a user account on the VM (if it does not exist) 
		and configures the necessary pair of SSH keys to enable file transfers and 
		execution. This account will be used to run the workflow;
	   
	\item \emph{Data Stage and Workflow Execution}: stages all the input data required 
		for the workflow execution on the VM, and launches the workflow execution. 
		Since our work is focused on infrastructure reproducibility, data and workflow 
		management are not covered in our approach. This part of the script is 
		generated ad-hoc for each workflow.
\end{itemize}

\noindent Note that all the configuration and deployment commands (first 3 sections) 
require superuser privileges on the VM. The workflow execution, however, is performed 
under the user account created in the third section.

\note{Rafael: continue to edit this section.}

\note{Idafen, modify this including the results of the 3 WFs, and how we check results are correct}.
\note{I've added some text explaining it, but we still have to agree on how results are checked for soykb and epigenomics}.

We executed the resultant scripts for the three workflows over their corresponding platforms. That is, a total of 9 different executions, as each workflow is executed in Futuregrid and Amazon EC2, using PRECIP, and also in a local Vagrant execution environment. All the executions where compared to their original one in a predefined VM image, where the execution environment was already in place. 

Results show that the VM execution environments deployed by all scripts are equivalent to the former ones. To check that not only the workflows are successfully executed but also that the results are equivalent we checked their output. In the case of Montage, which produces an image as output, we used a perceptual hash tool\footnote{pHash - \url{http://www.phash.org}} to compare the resulting image (0.1 degree image of the sky) against the one generated by the baseline execution, obtaining a similarity factor of 1.0 (over 1.0) with a threshold of 0.85.

For Epigenomics and SoyKB, where the output of the workflow is not deterministic enough to be checked using a hash method, we have manually verified that the result is equivalent. In both cases the results obtained in each infrastructure were equivalent in terms of their size and content.



All the original and generated scripts are available as part of the experimental material included in the Research Object (RO)~\cite{researchObjects} associated with this paper\footnote{\url{http://pegasus.isi.edu/publications/FGCS(CREATE-RO!)}}. This RO also contains pointers to the software and resources used in this experiment.





%\begin{listing}[!htb]
%\begin{minted}[linenos=true,fontsize=\scriptsize]{cpp}
%
%OPEN_SSH_CLIENTS_SOFT_STACK stack
% OPEN_SSH_CLIENTS_SOFT_COMP component
%  OPEN_SSH_CLIENTS_DEP_STEP step
%OPEN_SSH_SERVER_SOFT_STACK stack
% OPEN_SSH_SERVER_SOFT_COMP component
%  OPEN_SSH_SERVER_DEP_STEP step
%WGET_SOFT_STACK stack
% WGET_SOFT_COMP component
%  WGET_DEP_STEP step
%CONDOR_CENTOS_6_5_SOFT_STACK stack
% CONDOR_CENTOS_6_5_SOFT_COMP component
%  STOP_CONDOR_DEP_STEP step
%  ADD_CONDOR_REPO_DEP_STEP step
%  CONDOR_YUM_INSTALL_DEP_STEP step
%  CLEAN_AND_SET_CONDOR_DEP_STEP step
%  RESTART_DAEMONS_DEP_STEP step
%JAVA-1.7.0-OPENJDK.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK.X86_64_DEP_STEP step
%JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK-DEVEL.X86_64_DEP_STEP step
%PEGASUS_WMS_CENTOS_6_5_SOFT_STACK stack
% PEGASUS_WMS_CENTOS_6_5_SOFT_COMP component
%  ADD_PEGASUS_REPO_DEP_STEP step
%  PEGASUS_YUM_INSTALL_DEP_STEP step
%MFITPLANE_SOFT_STACK stack
% MFITPLANE_SOFT_COMP component
%  MFITPLANE_DEP_STEP step
%MDIFF_SOFT_STACK stack
% MDIFF_SOFT_COMP component
%  MDIFF_DEP_STEP step
%MDIFFFIT_SOFT_STACK stack
% MDIFFFIT_SOFT_COMP component
%  MDIFFFIT_DEP_STEP step
%MCONCATFIT_SOFT_STACK stack
% MCONCATFIT_SOFT_COMP component
%  MCONCATFIT_DEP_STEP step
%MIMGTBL_SOFT_STACK stack
% MIMGTBL_SOFT_COMP component
%  MIMGTBL_DEP_STEP step
%MSHRINK_SOFT_STACK stack
% MSHRINK_SOFT_COMP component
%  MSHRINK_DEP_STEP step
%MPROJECTPP_SOFT_STACK stack
% MPROJECTPP_SOFT_COMP component
%  MPROJECTPP_DEP_STEP step
%MADD_SOFT_STACK stack
% MADD_SOFT_COMP component
%  MADD_DEP_STEP step
%MBACKGROUND_SOFT_STACK stack
% MBACKGROUND_SOFT_COMP component
%  MBACKGROUND_DEP_STEP step
%MBGMODEL_SOFT_STACK stack
% MBGMODEL_SOFT_COMP component
%  MBGMODEL_DEP_STEP step
%MJPEG_SOFT_STACK stack
% MJPEG_SOFT_COMP component
%  MJPEG_DEP_STEP step
%
%\end{minted}
%\caption{Abstract deployment plan of the Montage WF}
%\label{lst:plan-montge}
%\end{listing}
%
%
%\begin{listing}[!htb]
%\begin{minted}[linenos=true,fontsize=\scriptsize]{cpp}
%
%OPEN_SSH_CLIENTS_SOFT_STACK stack
% OPEN_SSH_CLIENTS_SOFT_COMP component
%  OPEN_SSH_CLIENTS_DEP_STEP step
%OPEN_SSH_SERVER_SOFT_STACK stack
% OPEN_SSH_SERVER_SOFT_COMP component
%  OPEN_SSH_SERVER_DEP_STEP step
%WGET_SOFT_STACK stack
% WGET_SOFT_COMP component
%  WGET_DEP_STEP step
%CONDOR_CENTOS_6_5_SOFT_STACK stack
% CONDOR_CENTOS_6_5_SOFT_COMP component
%  STOP_CONDOR_DEP_STEP step
%  ADD_CONDOR_REPO_DEP_STEP step
%  CONDOR_YUM_INSTALL_DEP_STEP step
%  CLEAN_AND_SET_CONDOR_DEP_STEP step
%  RESTART_DAEMONS_DEP_STEP step
%JAVA-1.7.0-OPENJDK.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK.X86_64_DEP_STEP step
%JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_STACK stack
% JAVA-1.7.0-OPENJDK-DEVEL.X86_64_SOFT_COMP component
%  JAVA-1.7.0-OPENJDK-DEVEL.X86_64_DEP_STEP step
%PEGASUS_WMS_CENTOS_6_5_SOFT_STACK stack
% PEGASUS_WMS_CENTOS_6_5_SOFT_COMP component
%  ADD_PEGASUS_REPO_DEP_STEP step
%  PEGASUS_YUM_INSTALL_DEP_STEP step
%MAQ_INDEX_X86_64_SOFT_STACK stack
% MAQ_INDEX_X86_64_SOFT_COMP component (version: 1.0)
%  MAQ_INDEX_X86_64_DEP_STEP step
%  MAQ_INDEX_X86_64_2_DEP_STEP step
%MAQ_X86_64_SOFT_STACK stack
% MAQ_X86_64_SOFT_COMP component (version: 1.0)
%  MAQ_X86_64_DEP_STEP step
%  MAQ_X86_64_2_DEP_STEP step
%RPM_GLIBCi686_SOFT_STACK stack
% RPM_GLIBCi686_SOFT_COMP component
%  RPM_GLIBCi686_DEP_STEP step
%RPM_GLIBC_X86_64_SOFT_STACK stack
% RPM_GLIBC_X86_64_SOFT_COMP component
%  RPM_GLIBC_X86_64_DEP_STEP step
%RPM_COMPAT_LIBSTDCPP_33_X86_64_SOFT_STACK stack
% RPM_COMPAT_LIBSTDCPP_33_X86_64_SOFT_COMP component
%  RPM_COMPAT_LIBSTDCPP_33_X86_64_DEP_STEP step
%RPM_PERL_V5_14_4_X86_64_SOFT_STACK stack
% RPM_PERL_V5_14_4_X86_64_SOFT_COMP component
%  RPM_PERL_V5_14_4_X86_64_DEP_STEP step
%FASTQSPLIT_SOFT_STACK stack
% FASTQSPLIT_SOFT_COMP component (version: 1.0)
%  FASTQSPLIT_DEP_STEP step
%  FASTQSPLIT_2_DEP_STEP step
%SOL2SANGER_SOFT_STACK stack
% SOL2SANGER_SOFT_COMP component (version: 1.0)
%  SOL2SANGER_DEP_STEP step
%  SOL2SANGER_2_DEP_STEP step
%MAP_SOFT_STACK stack
% MAP_SOFT_COMP component (version: 1.0)
%  MAP_DEP_STEP step
%  MAP_2_DEP_STEP step
%CHR21_SOFT_STACK stack
% CHR21_SOFT_COMP component (version: 1.0)
%  CHR21_DEP_STEP step
%  CHR21_2_DEP_STEP step
%PILEUP_SOFT_STACK stack
% PILEUP_SOFT_COMP component (version: 1.0)
%  PILEUP_DEP_STEP step
%  PILEUP_2_DEP_STEP step
%FAST2BFQ_SOFT_STACK stack
% FAST2BFQ_SOFT_COMP component (version: 1.0)
%  FAST2BFQ_DEP_STEP step
%  FAST2BFQ_2_DEP_STEP step
%MAPMERGE_SOFT_STACK stack
% MAPMERGE_SOFT_COMP component (version: 1.0)
%  MAPMERGE_DEP_STEP step
%  MAPMERGE_2_DEP_STEP step
%FILTERCONTAMS_SOFT_STACK stack
% FILTERCONTAMS_SOFT_COMP component (version: 1.0)
%  FILTERCONTAMS_DEP_STEP step
%  FILTERCONTAMS_2_DEP_STEP step
%
%\end{minted}
%\caption{Abstract deployment plan of the Epigenomics WF}
%\label{lst:plan-epigenomics}
%\end{listing}