\section{Introduction}

Reproducibility of results of scientific experiments is a cornerstone of the scientific 
method. Therefore, the scientific community has been encouraging researchers 
to publish their contributions in a verifiable and understandable 
way~\cite{YaleRoundtable09, James-XSEDE-2014}. In computational science, 
or \emph{in-silico} science, reproducibility often requires that researchers make 
code and data publicly available so that the data can be analyzed in a similar 
manner as in the original work described in the publication. Code must be made available, 
and data must be accessible in a readable format~\cite{bookReproducibility}. 

Scientific workflows are a useful representation for managing the execution of large-scale 
computations. Many scientists now formulate their computational problems as scientific 
workflows running on distributed computing infrastructures such as campus Clusters, 
Clouds, and Grids~\cite{workflowBook}. Researchers in bioinformatics have 
embraced workflows for a whole range of analyses, including protein folding~\cite{craddock2006science}, 
DNA and RNA sequencing~\cite{blankenberg2010galaxy, giardine2005galaxy}, 
and disease-related research~\cite{fisher2009systematic, gaizauskas2004}, among others.
The workflow representation not only facilitates the creation and management of the 
computation but also builds a foundation upon which results can be validated and 
shared. 

Much research has focused on studying the reproducibility of scientific results 
in life sciences. Some studies clearly show the difficulties when trying to replicate 
experimental results in biology~\cite{Ioannidis2009}. The \emph{Reproducibility 
Project: Cancer Biology}~\cite{ErringtonCancerRerpoducibility} is an active project 
aiming to independently reproduce the experimental results of 50 high-impact cancer 
biology studies, evaluating the degree of reproducibility of those results and the main 
issues related to them.


Since workflows formally describe the sequence of computational and data 
management tasks, it is easy to trace the origin of the data produced. Many workflow 
systems capture provenance at runtime, what provides the lineage of data products 
and as such underpins scientific understanding and data reuse by providing the basis on which 
trust and understanding are built. A scientist would be able to look at the workflow and 
provenance data, retrace the steps, and arrive at the same data products. 
However, this information is not sufficient for achieving full reproducibility.

%\note{Idafen, better introduce here concepts such as: Reproducibility, replicability, conservation,...}

Reproducibility, replicability, and repeatability are often used as synonyms. Even when they
pursue similar goals, there are several differences between them~\cite{Drummond2011}. In this work we consider them
as separated concepts. While replicability can be defined as a
strict recreation of the original experiment, using the same method, data and equipment, reproducibility
implies that at least some changes have been introduced in the experiment, thus exposing 
different features.  While being a less restrictive term, reproducibility
is a key concept in science, as it allows incremental research 
by modifying, improving and repurposing the experimental methods and conditions. 

In this work, we address the reproducibility of the execution environment for a scientific workflow, 
as we do not aim to obtain an exact incarnation of the original setup, but rather an 
environment that is able to support the required capabilities exposed by the former 
environment. In order to reproduce or replicate any digital artifact we need to properly handle its conservation.
According to~\cite{King1995}, to achieve conservation one needs to guarantee that \emph{``sufficient information
exists with which to understand, evaluate, and build upon a prior work if a third party could replicate
 the results without any additional information from the author''}. Hence, we address workflow conservation
in order to attain its reproducibility.


In~\cite{Garijo2013}, authors explain the problems they faced when they tried to 
reproduce an experiment~\cite{drugomePrimer} for mapping all putative FDA 
and European drugs to protein receptors within the scope of a given proteome. For 
each identified problem, they enumerate a set of suggestions for addressing the related
issues. In four out of the total six cases, execution environment problems are mentioned.

Currently, most of the approaches in computational science conservation, in particular 
for scientific workflow executions, have been focused on data, code, and the workflow 
description, but not on the underlying infrastructure---which is composed of a set of 
computational resources (e.g., execution nodes, storage devices, and networking) and 
software components. We identify two approaches for conserving the environment of an 
experiment: 1) \emph{physical conservation}, where the real object is conserved due to 
its relevance and the difficulty in obtaining a counterpart; and 2) \emph{logical conservation}, 
where objects are described in such a way that an equivalent one can be obtained in a 
future experiment.

The computational environment is often conserved by using the physical approach, where 
computational resources are made available to scientists over a sustained period of time. 
As a result, scientists are able to reproduce their experiments in the same environment. 
However, such infrastructures demand huge maintenance efforts, and there is no guarantee 
that it will not change or suffer from a natural decay process~\cite{Gavish2011637}. 
Furthermore, the infrastructure may be subjected to organization policies, which restrict 
its access to a selective group of scientists, thereby limiting reproducibility to this restricted 
group. On the other hand, data, code, and workflow descriptions can be conserved by using 
a logical approach, which is not subjected to natural decay processes.

Accordingly, we propose a logical-oriented approach to conserve computational environments, 
where the capabilities of the resources (virtual machines (VM)) are described. From this 
description, any scientist, interested in reproducing an experiment, will be able to reconstruct 
the former infrastructure (or an equivalent one) in any Cloud computing infrastructure (either 
private or public). One may argue that it would be easier to keep and share VM images (or
containers) with the community research through a common repository, however the high storage 
demand of VM images (in particular for Big Data applications) may be a challenging 
problem~\cite{Mao:2014:ROD:2600090.2512348,6552826}. 

%\feedback{@Oscar: (following paragraph) big change in the story. Not sure about its usefulness}

Inspired by the aforementioned ideas, exposed in~\cite{King1995}, we aim to  define means 
for authors to share the relevant information about the execution environment of a given scientific
workflow. We argue that by explicitly describing this knowledge we increase the degree of 
reproducibility of the environment and of the workflow.

Semantics have been proposed as a way of attaining curation and conservation of the digital 
assets related to scientific experiments (e.g., biomedical research~\cite{MaloneSWO2014}). 
Our approach uses semantic-annotated workflow descriptions to generate lightweight scripts 
for an experiment management API that can reconstruct the required infrastructure. We 
propose to describe the resources involved in the execution of the experiment using a set of 
semantic vocabularies, and use those descriptions to define the infrastructure specification. 
This specification can then be used to derive the set of instructions that can be executed to 
obtain a new equivalent infrastructure. 

We conduct a practical experimentation process in which we describe a set 
of workflows and their environments using a set of semantic models. Then, we 
use an experiment management tool to reproduce a workflow execution in different 
Cloud platforms. In this work, we study three real and representative production 
workflows, which have been disseminated in several scientific publications. Many 
scientific workflows allow their execution infrastructure to be escalated, depending on 
the processing needs of the application. \rev{In this work, we analyze the reproduction 
of workflows which were formerly configured to run into single computer infrastructures.}

% work under a single node configuration

%I agree. Say that we used real production workflows
%actually, I think we could extend the paragraph "Semantics have been proposed as a way ..."

%focus on small, single-computer experiment

%focus on software rather than hardware

%we assume that we have a reliable network on our providers, so we do not model it, but we plan to do it on the future

% --- Not sure about this paragraph
%In this work, we entail reproducibility from the execution environment point of view. For the sake of showing how our approach works, we have also included the data and the workflow execution of the experiments we study in this paper.

The first iteration of the semantic models for representing computational 
resources, and some of the reproducibility tools were introduced and evaluated 
in~\cite{SantanaPerez-REPPAR-2014} for a single astronomy workflow. In this 
work, we extend our previous work by introducing 1)~a set of new features to 
our framework based on the result of our previous work, including a new 
version of our models and features in our algorithmic process; 2)~a study of 
two new life sciences workflows based on genomic processing applications 
 and their related challenges with respect to previous cases; and 3)~a 
practical evaluation of the framework with the new features for the astronomy 
workflow as well as to the new life science workflows. 


The paper is organized as follows. Section~\ref{sec:semantic} describes our semantic approach 
for documenting computational infrastructures. Section~\ref{sec:reproducibility} presents the 
description of the tools used to implement the semantic models and manage the experiment. 
Section~\ref{sec:experiment} describes the experimentation process. Section~\ref{sec:related-work} 
presents the related work, and Section~\ref{sec:conclusion} summarizes our results and 
identifies future works.


