\section{Related Work}
\label{sec:related-work}

A computational experiment involves several elements that must be conserved 
to ensure reproducibility. In the last year several studies and initiatives have been 
conducted for solving its associated challenges~\cite{Hothorn01052011,Sylwester14}. 
Most of the works addressed the conservation of data and the workflow description, 
however the computational environment is often neglected. Recent studies have 
exposed the necessity of publishing adequate descriptions of the runtime environment of experiments to avoid replication hindering~\cite{Rollins201459}. As a result, there is an increased focus on the number of publications 
providing associated experimental materials~\cite{Brown2012, diginorm}.

A study to evaluate reproducibility in scientific workflows was conducted in~\cite{zhao2012}. The study evaluated a set of domain-specific workflows, available in  myExperiment~\cite{myExperiment}, to identify causes of workflow decay. The study showed that nearly 80\% of the workflows cannot be reproduced, that about 12\% of these reproducibility issues are due to the lack of information about the execution environment, and that 50\% of them are due to the use of third-party resources such as web services and databases that are not available anymore. 

%Note that some of those third-party resource issues could be also considered as execution environment problems, as many of them are remote services for information processing. 
%\feedback{Ewa: Yes, but you don't deal with that I think.}

Recently, another comprehensive study has been published~\cite{Collberg2015}, where 601 papers from ACM conferences were surveyed, studying how authors share the data and code supporting their results. Authors found that 402 of those papers were supported by code. In this study authors tried to obtain the code related to each publication, looking for links within the paper itself, searching on code repositories, and contacting the authors when necessary. After the code was obtained, several students were asked to try to build it. This whole process was limited by experimental design to a period of 30 minutes. Results showed that in 32.3\% of the 402 papers students were able to obtain the code and build the code within the given period. In 48.3\% of the cases, code was built with some extra effort, and in 54\% of the papers code was either built or the authors stated the code would build with reasonable effort. Authors proposed, as a result of this study, a {\it sharing specification} for publications that allow to state the level of sharing of each paper.

The workflow paradigm has been widely adopted in the bioinformatics community, for studying genome sequencing ~\cite{blankenberg2010galaxy, giardine2005galaxy}, disease-related experiments~\cite{fisher2009systematic, gaizauskas2004} and many others. Several studies have exposed the difficulties of trying to reproduce experimental results on life sciences, such as biology~\cite{Ioannidis2009} and cancer analysis~\cite{ErringtonCancerRerpoducibility}.

Replicability and reproducibility of computational experiments using cloud computing resources and software descriptions have been proposed as an approach for those studies in which performance is not a key experimental result~\cite{Crick14}.

The Executable Paper Grand Challenge~\cite{elsevierchallenge} and the SIGMOD conference in 2011~\cite{SIGMOD} highlighted the importance of allowing the scientific community to reexamine experiment execution. The conservation of virtual machine (VM) images emerges as a way of preserving the execution environment~\cite{Brammer,SHARE}. However, the high storage demand of VM images remains a challenging problem~\cite{Mao:2014:ROD:2600090.2512348,6552826}. Moreover, the cost of storing and managing data in the Cloud is still high, and the execution of high-interactivity experiments through a network connection to remote virtual machines is also challenging. A list of advantages and challenges of using VMs for achieving reproducibility was exposed in~\cite{Howe2012}.

In~\cite{reprozip}, authors introduce ReproZip, a provenance-based 
tool for tracking the operating system calls to identify the libraries and 
data dependencies, as well as the configuration parameters involved 
in an experiment. The tool combines all these dependencies into a 
single package that can be used to reproduce an experiment. 
Although this approach avoids storing VM images, it still requires 
storing the application binaries and their dependencies. In this work, we
use semantic annotations to describe these dependencies. In our 
approach, we do not try to capture and package the original elements of 
the former infrastructure (copy the files and libraries). Instead, we describe 
them in a way that an available counterpart can be retrieved and tuned to 
expose the same characteristics. We claim that we do not only provide a 
technical solution, but also increase the reproducibility degree of the 
experiment by exposing the knowledge about the underlying infrastructure 
in a structured way. ReproZip would attain better dynamism if an 
approach as the one proposed in this work is used, since it abstracts 
the description of the infrastructure from the concrete elements that 
are involved in the original experiment.


%These approaches differ from ours as we do not try to capture the real elements of the infrastructure (copy the files and libraries) but rather we try to describe them and obtain an available counterpart that can be tuned to expose the same features. We agree with ReproZip authors that packaging the physical infrastructure components limits the scope of applicability, as the packages require most of the target machine to be the same. We also argue that the knowledge and understanding of the infrastructure, as well as the dynamism of this solution would be higher using an approach like the one exposed on our work, as we abstract the description of the infrastructure from the concrete elements that are involved in the former experiment.

%\feedback{@Maria: add more references to previous work on life science reproducibility}
%\note{Idafen: there are several references to this in the introduction. Should we repeat them here?}

\rev{Galaxy~\cite{goecks2010galaxy}, is a web-based WMS that aims to bring data 
analysis capabilities to non-expert users in the biological sciences domain, hiding the 
implementation  details of the underlying tools for workflow developers. These 
workflows can be deployed on a Galaxy server locally or on the cloud, through 
the Galaxy  CloudMan component. The main goals of the Galaxy framework 
are accessibility to biological computational  capabilities and reproducibility of the
analysis result by tracking the information  related to the full process. 
Even when this approach has proved to be successful in many cases,
it does not consider infrastructure aspects.} 



% Even when this approach has proved to be successful in several cases, we argue that it does not cover local development of workflows, which is a common case in computational science.  

%\todo{state that most galaxy wfs are developed usually using external services rather than local, and thus it does not fit into our REPRODUCIBILITY approach}


Software components cannot be preserved just by maintaining their binary executable code, but by guaranteeing the performance of their features. In~\cite{Matthews}, the concept of adequacy is introduced to measure how a software component behaves relative to a certain set of features. Our work is based on this same concept, where we build a conceptual model to semantically annotate the relevant properties of each software component. Then, we use scripting to reconstruct an equivalent computational environment using these annotations.



Some efforts have been undertaken to define syntaxes for describing the 
execution environment of scientific applications. The recently released 
TOSCA\footnote{http://docs.oasis-open.org/tosca/TOSCA/v1.0/os/TOSCA-v1.0-os.html} 
standard has been proposed to specify the components and life cycle of 
scientific workflows, including their dependencies. TOSCA is a Cloud 
management standard that creation of declarative templates 
for defining the workflow as well as the information required for enacting 
its environment. As a result, workflows are portable to any TOSCA-compliant 
provider~\cite{TOSCA}. Although this approach significantly increases the 
reproducibility degree of the experiment, it requires the workflow to be
rewritten. In this work, however, our goal is to describe the workflow to 
obtain an equivalent execution environment depending on the available 
providers.



Several tools have been developed to enact the supporting environment.
For instance, Cloudify\footnote{http://getcloudify.org/} (a tool based on the
TOSCA standard) allows to define the configuration of a given application 
and its dependencies. These descriptions can be later used to state the 
application's installation process as well as its lifecycle. Infrastructure 
Manager~\cite{IM2015} is a tool that adds an abstraction layer for selecting, 
deploying, and configuring VMs in several providers, such as OpenNebula~\cite{opennebula}, 
Microsoft Azure\footnote{http://azure.microsoft.com}, and OpenStack. In
this work, we explore PRECIP and Vagrant, but thanks to WICUS decoupled
architecture, other tools can be easily incorporated.
% making the target application cloud-independent. 
%In this work we explore different tools, such as PRECIP and Vagrant, for enacting the supporting environment. Further studied should study other well-known solutions. Based on the aforementioned TOSCA standard, 
%These technologies and standards are compatible with our approach, and they could be integrated into our system as new enactment and specification solutions.



A recent and relevant contribution to the state of the art of workflow preservation was developed within the context of the TIMBUS project~\cite{Mayer2014Ontologies}. The project  aimed to preserve and ensure the availability of business processes and their computational infrastructure, aligned with the enterprise risk and the business continuity management. They also proposed a semantic approach for describing the execution environment of a process.  However, even though TIMBUS has studied the applicability of their approach to the eScience domain, their approach is mainly focused on business processes.

Semantics have also been proposed in the area of biomedical research 
as a way for achieving reproducibility of published experiments. 

For instance, in~\cite{MaloneSWO2014} authors propose a taxonomy for 
classifying software artifacts in the context of biological computations in 
the same way that gene products or phenotypes are classified. To build the
classification, they developed the {\it Software Ontology} (SWO), a model 
for cataloguing the software involved on the storage and management of data.
The SWO is a comprehensive catalogue of software-related concepts for 
composing description of processing artifacts. It contains more than 4000 
classes distributed in several modules for describing the different areas of 
software information. Many of these concepts share commonalities with the 
concepts used in the WICUS network, in particular on the \texttt{Software 
Domain} ontology. We identify a conceptual equivalences between 
\path{swo:Software} and \path{wstack:SotfwareComponent} classes, or 
\path{swo:has_version} and \path{wstack:hasVersion}, even though their 
context and purpose are not the same. While the SWO ontology focuses 
on describing the properties of a biology software, aiming to make it more 
understandable and potentially reproducible, the WICUS ontologies focus 
on the deployment of software artifacts and stating their necessary 
dependencies. We aim to develop a more generic and flexible ontology 
that can be applied in different scientific domains.

The EDAM ontology\footnote{http://edamontology.org/page} is a more 
concise ontology for describing bioinformatics resources. It includes concepts 
for annotating different data types, identifiers and formats, as well as topics 
and operations. Even when concept of operations can be related to the 
concept of software components, they are more related to data 
transformations rather than to executable deployment and computations.
The ontologies proposed in this work are not strictly focused on data 
descriptions and analysis.



A plethora of models have also been proposed to describe Cloud 
environments~\cite{Youseff2008}. Many initiatives have been undertaken to 
define Cloud Computing interfaces, such as the Open Cloud Computing 
Interface (OCCI\footnote{http://occi-wg.org}), one of the pioneering initiatives, 
and Unified Cloud Interface (UCI\footnote{http://code.google.com/p/unifiedcloud}), 
which provides several ontologies describing Cloud infrastructures besides defining 
an interface. In~\cite{Hoefer2010}, Cloud taxonomies for describing services and 
providers from the three main layers of Cloud Computing are proposed. The 
CoCoOn ontology~\cite{zhang2012ontology} describes the main terms of 
Infrastructure as a Service (IaaS) providers, including functional (e.g., memory 
or pricing) and non-functional (e.g., quality of service) concepts. Even tough
we characterize the main properties of computational resources, which belong 
to the IaaS layer, we do not aim to model the Cloud providers themselves neither 
the quality of services provided by them. We acknowledge that those considerations 
are relevant for the scheduling problem, however they are out of the scope of 
this work.
















