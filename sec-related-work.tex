\section{Related Work}
\label{sec:related-work}

A computational experiment involves several elements that must be conserved 
to ensure reproducibility. In the last year several studies and initiatives have been 
conducted for solving its associated challenges~\cite{Hothorn01052011,Sylwester14}. 
Most of the works addressed the conservation of data and the workflow description, 
however the computational environment is often neglected. Recent studies have 
exposed the necessity of publishing adequate descriptions of the runtime environment of experiments to avoid replication hindering~\cite{Rollins201459}. As a result, there is an increased focus on the number of publications 
providing associated experimental materials~\cite{Brown2012, diginorm}.

A study to evaluate reproducibility in scientific workflows was conducted in~\cite{zhao2012}. The study evaluated a set of domain-specific workflows, available in  myExperiment~\cite{myExperiment}, to identify causes of workflow decay. The study showed that nearly 80\% of the workflows cannot be reproduced, that about 12\% of these reproducibility issues are due to the lack of information about the execution environment, and that 50\% of them are due to the use of third-party resources such as web services and databases that are not available anymore. 

%Note that some of those third-party resource issues could be also considered as execution environment problems, as many of them are remote services for information processing. 
%\feedback{Ewa: Yes, but you don't deal with that I think.}

Recently, another comprehensive study has been published~\cite{Collberg2015}, where 601 papers from ACM conferences were surveyed, studying how authors share the data and code supporting their results. Authors found that 402 of those papers were supported by code. In this study authors tried to obtain the code related to each publication, looking for links within the paper itself, searching on code repositories, and contacting the authors when necessary. After the code was obtained, several students were asked to try to build it. This whole process was limited by experimental design to a period of 30 minutes. Results showed that in 32.3\% of the 402 papers students were able to obtain the code and build the code within the given period. In 48.3\% of the cases, code was built with some extra effort, and in 54\% of the papers code was either built or the authors stated the code would build with reasonable effort. Authors proposed, as a result of this study, a {\it sharing specification} for publications that allow to state the level of sharing of each paper.

The workflow paradigm has been widely adopted in the bioinformatics community, for studying genome sequencing ~\cite{blankenberg2010galaxy, giardine2005galaxy}, disease-related experiments~\cite{fisher2009systematic, gaizauskas2004} and many others. Several studies have exposed the difficulties of trying to reproduce experimental results on life sciences, such as biology~\cite{Ioannidis2009} and cancer analysis~\cite{ErringtonCancerRerpoducibility}.

Replicability and reproducibility of computational experiments using cloud computing resources and software descriptions have been proposed as an approach for those studies in which performance is not a key experimental result~\cite{Crick14}.

The Executable Paper Grand Challenge~\cite{elsevierchallenge} and the SIGMOD conference in 2011~\cite{SIGMOD} highlighted the importance of allowing the scientific community to reexamine experiment execution. The conservation of virtual machine (VM) images emerges as a way of preserving the execution environment~\cite{Brammer,SHARE}. However, the high storage demand of VM images remains a challenging problem~\cite{Mao:2014:ROD:2600090.2512348,6552826}. Moreover, the cost of storing and managing data in the Cloud is still high, and the execution of high-interactivity experiments through a network connection to remote virtual machines is also challenging. A list of advantages and challenges of using VMs for achieving reproducibility was exposed in~\cite{Howe2012}.

In~\cite{reprozip}, authors introduce ReproZip, a provenance-based tool for tracking the operating system calls to identify the libraries and data dependencies, as well as the configuration parameters involved in an experiment. The tool combines all these dependencies into a single package that can be used to reproduce an experiment. \rev{Although this approach avoids storing VM images, it still requires storing the application binaries and their dependencies. Instead, our work uses semantic annotations to describe these dependencies. In our approach we are not trying to capture and package the original elements of the former infrastructure (copy the files and libraries). We are rather describing them in a way that an available counterpart can be retrieved and tuned to expose the same characteristics. In this work we claim that we are not only providing a technical solution, but also increasing the reproducibility degree of the experiment by exposing the knowledge about the underlaying infrastructure in an structured way. The dynamism of this solution would be higher using an approach like the one exposed on our work, as we abstract the description of the infrastructure from the concrete elements that are involved in the former experiment}


%These approaches differ from ours as we do not try to capture the real elements of the infrastructure (copy the files and libraries) but rather we try to describe them and obtain an available counterpart that can be tuned to expose the same features. We agree with ReproZip authors that packaging the physical infrastructure components limits the scope of applicability, as the packages require most of the target machine to be the same. We also argue that the knowledge and understanding of the infrastructure, as well as the dynamism of this solution would be higher using an approach like the one exposed on our work, as we abstract the description of the infrastructure from the concrete elements that are involved in the former experiment.

%\feedback{@Maria: add more references to previous work on life science reproducibility}
%\note{Idafen: there are several references to this in the introduction. Should we repeat them here?}

Galaxy~\cite{goecks2010galaxy}, a well known workflow system in the context of life sciences, introduced a web-service based system for achieving accessible and reproducible computations, hiding the implementation details of the underlying tools for workflow developers, providing a web-based interface for retrieving an analyzing genomic data. \rev{This approach has proved to be successful in several cases and, even when it works in a local execution scenario, it is mostly used for developing workflows with external services dependencies. Thus, it is not so suitable for our reproducibility approach, which mainly focused on workflows without this kind of dependencies, as they are out of workflow developer control.}


% Even when this approach has proved to be successful in several cases, we argue that it does not cover local development of workflows, which is a common case in computational science.  

%\todo{state that most galaxy wfs are developed usually using external services rather than local, and thus it does not fit into our REPRODUCIBILITY approach}


Software components cannot be preserved just by maintaining their binary executable code, but by guaranteeing the performance of their features. In~\cite{Matthews}, the concept of adequacy is introduced to measure how a software component behaves relative to a certain set of features. Our work is based on this same concept, where we build a conceptual model to semantically annotate the relevant properties of each software component. Then, we use scripting to reconstruct an equivalent computational environment using these annotations.

A recent and relevant contribution to the state of the art of workflow preservation was developed within the context of the TIMBUS project~\cite{Mayer2014Ontologies}. The project  aimed to preserve and ensure the availability of business processes and their computational infrastructure, aligned with the enterprise risk and the business continuity management. They also proposed a semantic approach for describing the execution environment of a process.  However, even though TIMBUS has studied the applicability of their approach to the eScience domain, their approach is mainly focused on business processes.

Semantics have been also proposed in the area of biomedical research as a way for achieving reproducibility of published experiments~\cite{MaloneSWO2014}. In this work authors \rev{ propose a taxonomy for classifying software artifacts in the context of biological computations, in the same way that gene products or phenotypes are classified. In order to do so, authors developed the {\it Software Ontology} (SWO), a model for cataloguing the software involved on the storage and management of data.

The SWO is a comprehensive catalogue of software-related concepts for composing description of processing artifacts. It contains more than 4000 classes distributed in several modules for describing the different areas of software information Many of this concepts share commonalities with the concepts on the WICUS network, specially with its {\it Software Domain} ontology. We can identify a conceptual equivalences between \path{swo:Software} and \path{wstack:SotfwareComponent} classes or \path{swo:has_version} and \path{wstack:hasVersion}, even tough their context and purpose is not the same. While the SWO ontology focuses on describing the properties of biology software, aiming to make it more understandable and potentially reproducible, the WICUS ontologies focus on the deployment of software artifacts and stating their necessary dependencies. We aim to develop a more generic and flexible ontology that can be applied in different scientific domains.

Another related approach, the EDAM ontology~\cite{XXX}, is a more concise ontology for describing bioinformatics resources. It includes concepts for annotating different data types, identifiers and formats, as well as topics and operations. Even when concept of operations can be related to the concept of software components, they are more related to data transformations than to executable deployment and computations. The ontologies we are proposing in this work are not directly focused to data descriptions and analysis.

}

\rev{
Several models have been also proposed to describe Cloud environments~\cite{Youseff2008}. Several initiatives have been undertaken to define Cloud Computing interfaces, such as the Open Cloud Computing Interface (OCCI\footnote{http://occi-wg.org/}), one of the pioneering initiatives in the area, and Unified Cloud Interface (UCI\footnote{http://code.google.com/p/unifiedcloud/}), that defines not only an interface but also provides several ontologies describing cloud infrastructures. Cloud taxonomies for describing services and providers from the three main layers of Cloud Computing have been proposed also~\cite{Hoefer2010}. The CoCoOn ontology~\cite{zhang2012ontology} describes the main terms of Infrastructure as a Service (IaaS) providers, including functional concepts such as memory or pricing, and non-functional ones, such as Quality of Service (QoS) information. 

Even when we characterize the main properties of computational resources, belonging to the IaaS layer, we do not aim to model the Cloud providers themselves neither their quality of the services they provide. Those considerations, even  though they are interesting for scheduling purposes, are out of the scope of this work.
}




\note{mygrid}
%The myGrid Ontology [23], though focusing on bioinformatics software resources, makes many of the same distinctions as the SWO, but has not been actively maintained for a long-time. EDAM covers the concepts included in myGrid and, with the SWO, supersedes and replaces the myGrid ontology.
%\feedback{Ewa: there is a lot of problems with tense. Sometimes is in the past, sometimes in the present. Revise it.}














